{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification \n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.models import EModel, Wav2Vec2Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/joono/anaconda3/envs/torch/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/work/joono/anaconda3/envs/torch/lib/python3.12/site-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Trainable Lora layers ===========\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([768, 8])\n",
      "torch.Size([256, 768])\n",
      "torch.Size([256])\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2])\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"facebook/wav2vec2-base\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "\n",
    "path = \"/home/work/joono/joono/joono/DV_DV.Deep/w2cs5agc/checkpoints/best-checkpoint_oneshot.ckpt\"\n",
    "model = Wav2Vec2Facebook.load_from_checkpoint(path, args={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path):\n",
    "    audio, _ = librosa.load(path, sr=sampling_rate)\n",
    "    inputs = feature_extractor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    return inputs.input_values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate 함수 정의\n",
    "def collate_fn(batch):\n",
    "    signals, labels = zip(*batch)\n",
    "    max_length = max([signal.size(0) for signal in signals])\n",
    "    padded_signals = torch.zeros(len(signals), max_length)\n",
    "    for i, signal in enumerate(signals):\n",
    "        padded_signals[i, :signal.size(0)] = signal\n",
    "    labels = torch.tensor(labels)\n",
    "    return padded_signals, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(\"..\", \"dataset\", self.df.loc[idx, 'path'])\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"파일을 찾을 수 없습니다: {path}\")     \n",
    "        signal = speech_file_to_array_fn(path)           \n",
    "        return signal, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../dataset/test.csv', index_col=None)\n",
    "test_df['path'] = '../dataset/' + test_df['path'].str[1:]\n",
    "test_dataset = TestDataset(test_df)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, num_workers=24, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model = model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            probs = model(inputs)\n",
    "\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]/home/work/joono/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "100%|██████████| 1563/1563 [00:49<00:00, 31.38it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = inference(model=model, test_loader=test_loader, device='cuda:0')\n",
    "# preds = model.inference(test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_3057227/951011515.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.06928792595863342' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  else                            : submit.iloc[i, 1] = preds[i][0]\n",
      "/tmp/ipykernel_3057227/951011515.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9303889274597168' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  else                            : submit.iloc[i, 2] = preds[i][1]\n",
      "100%|██████████| 50000/50000 [00:08<00:00, 6147.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fake</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>0.069288</td>\n",
       "      <td>0.930389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>0.078578</td>\n",
       "      <td>0.920674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>0.072870</td>\n",
       "      <td>0.924842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>0.061994</td>\n",
       "      <td>0.943236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>0.090725</td>\n",
       "      <td>0.906439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      fake      real\n",
       "0  TEST_00000  0.069288  0.930389\n",
       "1  TEST_00001  0.078578  0.920674\n",
       "2  TEST_00002  0.072870  0.924842\n",
       "3  TEST_00003  0.061994  0.943236\n",
       "4  TEST_00004  0.090725  0.906439"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv('/home/work/joono/joono/dataset/sample_submission.csv')\n",
    "\n",
    "max_thres = 0.999\n",
    "min_thres = 0.001\n",
    "\n",
    "for i in tqdm(range(len(preds))):\n",
    "    if      preds[i][0] > max_thres : submit.iloc[i, 1] = 1\n",
    "    elif    preds[i][0] < min_thres : submit.iloc[i, 1] = 0 \n",
    "    else                            : submit.iloc[i, 1] = preds[i][0]\n",
    "    if      preds[i][1] > max_thres : submit.iloc[i, 2] = 1\n",
    "    elif    preds[i][1] < min_thres : submit.iloc[i, 2] = 0 \n",
    "    else                            : submit.iloc[i, 2] = preds[i][1]\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('joono_wav2vec2_test_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
